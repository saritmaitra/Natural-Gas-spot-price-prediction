{"cells":[{"metadata":{"_uuid":"861198b9-2bf7-4d67-a298-6c5199448cab","_cell_guid":"479bcf76-d81b-4de3-be8c-d2c8cfcc41f3","trusted":true},"cell_type":"code","source":"# %% [code]\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n# %% [code]\n!pip install probscale\nimport pywt\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm  \nfrom datetime import datetime\nimport math\nimport scipy as sp\nimport sklearn\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 150)\nimport time\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nimport pandas_profiling as pp\nfrom pandas.plotting import register_matplotlib_converters\nregister_matplotlib_converters()\nfrom scipy import stats\nimport numpy\nimport probscale\nimport seaborn\nimport math\nfrom math import log\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant\nfrom xgboost import XGBRegressor, XGBClassifier\nimport xgboost as xgb\nimport datetime\nfrom pandas.plotting import lag_plot\nfrom matplotlib import style\n%matplotlib inline\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\n!pip install pyforest\nfrom pyforest import *\nimport pandas_datareader as web\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 150)\nplt.style.use('ggplot')\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\n\n# %% [code]\npip install --upgrade pip\n\n# %% [code]\ndata = pd.read_excel(\"/kaggle/input/natural-gas/Natutalgas.xlsx\", parse_dates=True)\ndata.head()\n\n# %% [code]\ndata = data.rename(columns=data.iloc[0]).drop(data.index[0])\ndata.head()\n\n# %% [code]\ndata['Date'] = pd.to_datetime(data['Date'], errors='coerce')\ndf = data.set_index(data.columns[0])\ndf = df.sort_index(ascending=True)\ndf.head()\n\n# %% [code]\nplt.style.use('dark_background')\ndf['hh_sp'].plot(figsize = (15,5), grid=True)\nplt.title('Henry Hub Spot Price')\nplt.ylabel('Price(Dollars per Million Btu)')\nplt.show()\n\n# %% [code]\nplt.style.use('dark_background')\n# Plot\nfig, axes = plt.subplots(1, 4, figsize=(10,3), sharex=True, sharey=True, dpi=100)\nfor i, ax in enumerate(axes.flatten()[:4]):\n    lag_plot(df, lag=i+1, ax=ax)\n    ax.set_title('Lag ' + str(i+1))\n\nfig.suptitle('Lag Plots of Natural Gas \\n(Points get wide and scattered with increasing lag -> lesser correlation)\\n', y=1.15)    \nplt.show()\n\n# %% [code]\nfrom pandas.plotting import autocorrelation_plot\nautocorrelation_plot(df.hh_sp)\nplt.show()\n\n# %% [code]\ndf.dtypes\n\n# %% [code]\ndf = df.astype('float')\ndf.info()\n\n# %% [code]\ndf['return'] = df['hh_sp'].pct_change() * 100\ndf.shape\n\n# %% [code]\nplt.plot(df['return'])\n\n# %% [code]\nplt.figure()\nplt.subplot(211)\nplot_acf(df['hh_sp'], ax=plt.gca())\nplt.subplot(212)\nplot_pacf(df['hh_sp'], ax=plt.gca())\nplt.xlabel('Lag')\nplt.tight_layout()\nplt.show()\n\n# %% [code]\nnp.isnan(df).sum()\n\n# %% [code]\ndef fill_missing(df):\n    for row in range(df.shape[0]):\n        for col in range(df.shape[1]):\n            if np.isnan(df[row,col]):\n                df[row,col]= df[row-1, col]\n\nfill_missing(df.values)\nnp.isnan(df).sum()\n\n# %% [code]\nfill_missing(df.values)\nnp.isnan(df).sum()\n\n# %% [code]\nimport seaborn as sns\nsns.despine(left=True)\n\n# Plot a simple histogram with binsize determined automatically\nsns.distplot(df['hh_sp'], color=\"b\")\nplt.title('Henry hub spot price')\nplt.grid(True)\nplt.show()\n\n# %% [code]\nfrom scipy import stats\nstats.jarque_bera(df['hh_sp'])\n\n# %% [code]\nfig, ax = plt.subplots(figsize=(10,4))\nplt.grid(True)\nfig = probscale.probplot(df['hh_sp'], ax=ax, plottype='pp', bestfit=True,\n                         problabel='Percentile', datalabel='HenryHub spot price (S/MMBtu)',\n                         scatter_kws=dict(label='HH original data'),\n                         line_kws=dict(label='Best-fit line'))\nax.legend(loc='upper left')\nseaborn.despine()\n\n# %% [code]\nfig, ax = plt.subplots(figsize=(10,4))\nplt.grid(True)\nfig = probscale.probplot(df['hh_sp'], ax=ax, plottype='pp', bestfit=True,\n                         datascale='log',\n                         problabel='Percentile', \n                         datalabel='Log of HenryHub price ($/MMBtu)',\n                         scatter_kws=dict(label='HH lognormal data'),\n                         line_kws=dict(label='Best-fit line'))\nax.legend(loc='upper left')\nseaborn.despine()\n\n# %% [code]\ndf.columns\n\n# %% [code]\ndf.tail(2)\n\n# %% [markdown]\n# - Target variable is +1 if the spot price tomorrow is higher than the spot price today, and -1 if the spot price tomorrow is lower than the spot price today.\n# - we assume that the spot price tomorrow is not the same as the spot price today, which we can choose to handle by creating a third categorical value, 0.\n\n# %% [code]\ndclas = df.copy()\ndclas = dclas.loc[:, ~ dclas.columns.duplicated()] # drop duplicated columns if any\ndclas['target'] = np.where(dclas['hh_sp'].shift(-1) > dclas['hh_sp'], 1, -1)\ndclas.dropna(inplace=True)\n# classification set\n\nlags = 3\n# Create the shifted lag series of prior trading period close values\nfor i in range(0, lags):\n    dclas[\"Lag%s\" % str(i+1)] = dclas[\"hh_sp\"].shift(i+1).pct_change()\ndclas = dclas.dropna()\nx = dclas.drop(['hh_sp', 'target'], 1)\ny_clas = dclas.target\n\n# %% [code]\n# Checking Correlation \nsns.set(style='darkgrid', context='talk', palette='Dark2')\nplt.figure(figsize=(14,5))\ndclas.corr()['target'].sort_values(ascending = False).plot(kind='bar')\nplt.show()\n\n# %% [code]\nimport seaborn as sns\nsns.countplot(x = 'target', data=dclas, hue='target')\nplt.show()\n\n\n# %% [code]\nfrom sklearn.preprocessing import StandardScaler\n#Initialize and fit scaler\nscaler = StandardScaler()\n#Fit scaler using the training data\nscaler.fit(x)\n\n#Transform the raw data\nx_st = scaler.transform(x)\nprint(\"Number of features: \", x_st.shape[1])\n\n# %% [code]\nfrom sklearn.feature_selection import RFECV,SelectPercentile,mutual_info_classif, SelectKBest, f_classif\n# FEATURE SELECTION\n#1\n#y = y_clas\nmi_select = SelectPercentile(mutual_info_classif, \n                             percentile=60).fit(x_st,  np.ravel(y_clas))\n# remove lower 40%\nx_mi = mi_select.transform(x_st)\nprint(\"Feature Selection Results:\")\nprint(\"Filter Result:\"); print(\"Number of features: \", x_mi.shape[1])\n\n# %% [code]\n#2\n# Corr_threshold is the cuttoff level for correlation coefficient\ndef correlation(x_tr, corr_threshold):\n    corr = x.corr(method = \"spearman\").abs()\n    \n    \"\"\"Spearmanâ€™s correlation is non-parametric and does not assume a linear relationship between variables; \n    it looks for monotonic relationships\"\"\"\n\n    upper = corr.where(np.triu(np.ones(corr.shape), k = 1).astype(np.bool))\n    to_drop = [column for column in upper.columns if any(upper[column] > corr_threshold)]\n    x_corr_filtered = x.drop(to_drop, axis = 1)\n    return x_corr_filtered\n\ndef heat_map(corr,title):\n    sns.set(font_scale = 1.0)\n    f, ax = plt.subplots(figsize=(11, 9))\n    sns.heatmap(corr, cmap= \"YlGnBu\", square=True, ax = ax)\n    f.tight_layout()\n    ax.set_title(title)\n\nmi = pd.DataFrame(x_mi,\n                     index = x.index,\n                     columns = x.columns[mi_select.get_support()])\n\nx_corr = correlation(mi, 0.95)\n\n\"\"\"Spearman correlation coefficient is based on the ranked values for each variable\"\"\" \ncorr_before = mi.corr(method = \"spearman\").abs()\ncorr_after = x_corr.corr(method = \"spearman\").abs()\n\nheat_map(corr_before, \"Heat Map (BEFORE FILTERING)\")\nheat_map(corr_after, \"Heat Map (AFTER FILTERING)\")\n\nprint(\"Correlation Filter Result:\")\nprint(\"Number of features: \", x_corr.shape[1])\n\n# %% [code]\n!pip install pca\nfrom pca import pca\nfrom sklearn.decomposition import PCA\n\n# %% [code]\n#3\n#PCA\nplt.style.use('seaborn-whitegrid')\n# Initialize to reduce the data up to the number of componentes that explains 99% of the variance.\nmodel = pca(n_components = 0.99)\n# Fit transform\nx_pca = model.fit_transform(x_corr)\n# Plot explained variance\nfig, ax = model.plot()\n\ntot_var = 0.99 # total variance\nx_pca = PCA(tot_var, svd_solver = 'full').fit(x_corr)\nprint(x_pca.explained_variance_ratio_); print()\nprint(\"Reduced dimensions can explain {:.4f}\".format(sum(x_pca.explained_variance_ratio_)),\n      \"% of the variance in the original data\"); print()\nprint(x_pca.components_.shape[0]); print()\n# number of components\nn_pcs= x_pca.components_.shape[0]\n# get the index of the most important feature on EACH component\nmost_important = [np.abs(x_pca.components_[i]).argmax() for i in range(n_pcs)]\ninitial_feature_names = x_corr.columns\n# get the names\nmost_important_names = [initial_feature_names[most_important[i]] for i in range(n_pcs)]\ndic = {'PC{}'.format(i): most_important_names[i] for i in range(n_pcs)}\np= pd.DataFrame(dic.items())\nprint(p); print()\n\npca = PCA(tot_var, svd_solver = 'full')\nx_pca = pca.fit_transform(x_corr)\n\n\n# %% [code]\nfrom sklearn.model_selection import train_test_split\nSplitFactor = 0.8\nxTrain, xTest, yTrain, yTest = train_test_split(x_pca, y_clas, shuffle=False,\n                                                train_size = SplitFactor)\n\n# %% [code]\n# Create models\nprint(\"Accuracy scores/Confusion Matrices:\\n\")\nmodels = [(\"LR\", LogisticRegression()),\n          (\"LDA\", LinearDiscriminantAnalysis()),\n          (\"QDA\", QuadraticDiscriminantAnalysis()),\n          (\"LSVC\", LinearSVC()),\n          (\"RSVM\", SVC(C=1000000.0, cache_size=200, class_weight=None,\n                       coef0=0.0, degree=3, gamma=0.0001, kernel='rbf',\n                       max_iter=-1, probability=False, random_state=None,\n                       shrinking=True, tol=0.001, verbose=False)),\n          (\"RF\", RandomForestClassifier(\n              n_estimators=100, criterion='gini',\n              max_depth=None, min_samples_split=2,\n              min_samples_leaf=1, max_features='auto',\n              bootstrap=True, oob_score=False, n_jobs=1,\n              random_state=None, verbose=0))]\n# iterate over the models\nfor m in models:\n    # Train each of the models on the training set\n    m[1].fit(xTrain, yTrain)\n    # predictions on the test set\n    pred = m[1].predict(xTest)\n    # Accuracy Score and the confusion matrix for each model\n    print(\"%s:\\n%0.3f\" % (m[0], m[1].score(xTest, yTest)))\n    print(\"%s\\n\" % confusion_matrix(pred, yTest))\n\n# %% [code]\nrfc = RandomForestClassifier(\n              n_estimators=100, criterion='gini',\n              max_depth=None, min_samples_split=2,\n              min_samples_leaf=1, max_features='auto',\n              bootstrap=True, oob_score=False, n_jobs=1,\n              random_state=None, verbose=0).fit(xTrain, yTrain)\n\npd.set_option('float_format', '{:f}'.format)\ntest_pred = rfc.predict(xTest)\nprint('Test prediction values:')\ntest_pred = pd.DataFrame(test_pred)\ntest_pred.rename(columns = {0: 'TestPrediction'}, inplace=True); \ntest_pred.index = yTest.index\ncompare = pd.concat([yTest, test_pred], 1)\nprint(compare)\n\n# %% [code]\nyTest.shape\n\n# %% [code]\n# collect last 100 positions for visualization\ndd = pd.DataFrame(df.hh_sp[-45:])\ncombine = pd.concat([dd, compare],1)\ncombine\n\n# %% [code]\nprint('Actual value counts:')\nprint(compare['target'].value_counts())\nprint('Predicted value counts:')\nprint(compare['TestPrediction'].value_counts())\n\n# %% [markdown]\n# ## Regression\n\n# %% [code]\ndreg = df.copy()\ndreg['target'] = dreg['hh_sp'].shift(-1) - dreg['hh_sp']\ndreg.dropna(inplace=True)\ndreg.tail() \n\n# %% [code]\ndreg.shape\n\n# %% [code]\nlags = 3\n# Create the shifted lag series of prior trading period close values\nfor i in range(0, lags):\n    dreg[\"Lag%s\" % str(i+1)] = dreg[\"hh_sp\"].shift(i+1).pct_change()\ndreg = dreg.dropna()\nx_reg = dreg.drop(['hh_sp', 'target'],1)\ny_reg = dreg.target\n\n# %% [code]\nprint(x_reg.shape); print(y_reg.shape)\n\n# %% [code]\nfrom pca import pca\nfrom sklearn.decomposition import PCA\n\n# %% [code]\n#Initialize and fit scaler\nscaler = StandardScaler()\n#Fit scaler using the training data\nscaler.fit(x_reg)\n\n#Transform the raw data\nxScaler = scaler.transform(x_reg)\nprint(\"Number of original features: \", xScaler.shape[1])\n\nfrom sklearn.feature_selection import mutual_info_regression\nMiSelect = SelectPercentile(mutual_info_regression, \n                             percentile=60).fit(xScaler,y_reg.values)\n# 1 (mutual information)\nxMi = MiSelect.transform(xScaler)\nprint(\"Mutual information Feature Selection Results:\")\nprint(\"Filter Result:\"); print(\"Number of features: \", xMi.shape[1])\n\nMI = pd.DataFrame(xMi,\n                     index = x_reg.index,\n                     columns = x_reg.columns[MiSelect.get_support()])\n\n# 2 (correlation)\n# Corr_threshold is the cuttoff level for correlation coefficient\ndef correlation(x_reg, corr_threshold):\n    corr = x_reg.corr(method = \"spearman\").abs()\n    \n    \"\"\"Spearmanâ€™s correlation is non-parametric and does not assume a linear relationship between variables; \n    it looks for monotonic relationships\"\"\"\n\n    upper = corr.where(np.triu(np.ones(corr.shape), k = 1).astype(np.bool))\n    to_drop = [column for column in upper.columns if any(upper[column] > corr_threshold)]\n    x_corr_filtered = x_reg.drop(to_drop, axis = 1)\n    return x_corr_filtered\n\nMi = pd.DataFrame(xMi,\n                     index = x_reg.index,\n                     columns = x_reg.columns[mi_select.get_support()])\n\nxCorr = correlation(Mi, 0.95)\nprint(\"Correlation Filter Result:\")\nprint(\"Number of features: \", xCorr.shape[1])\n\n\n# 3 (principal component analysis)\ntot_var = 0.99 # total variance\npca = PCA(tot_var, svd_solver = 'full')\nxPca = pca.fit_transform(xCorr)\n\nfrom sklearn.model_selection import train_test_split\nSplitFactor = 0.8\nxTrain, xTest, yTrain, yTest = train_test_split(xPca, y_reg, shuffle=False,\n                                                train_size = SplitFactor)\n\nrfc = RandomForestRegressor().fit(xTrain, yTrain)\npd.set_option('float_format', '{:f}'.format)\ntest_pred = rfc.predict(xTest)\n\n# The mean squared error\nprint(\"Mean squared error (train): %.2f\"% mean_squared_error(yTrain, \n                                                            rfc.predict(xTrain)))\n# Explained variance score: 1 is perfect prediction\nprint('Variance score (train): %.2f' % r2_score(yTrain, \n                                                rfc.predict(xTrain)))\n# The mean squared error\nprint(\"Mean squared error (test): %.2f\" % mean_squared_error(yTest, \n                                                             rfc.predict(xTest)))\n# Explained variance score: 1 is perfect prediction\nprint('Variance score (test): %.2f' % r2_score(yTest, rfc.predict(xTest)))\n\n# %% [markdown]\n# R^2 value close to 1 means better model fit, and the closer the value to 0, the worse the fit. Negative values mean that the regression model fits worse than the baseline model. Models with negative R^2 values usually indicate issues in the training data or process and cannot be used.","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}